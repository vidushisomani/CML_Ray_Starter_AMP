{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cc082d-5ee6-45d1-8990-b646411a763a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c537c01-2313-42d2-a58c-c48f6b178477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9135d364-57df-4523-ac43-79b56588fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NCCL_SHM_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a907d335-eeed-4f81-82fc-65cc47649dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a094e6b1-5e6c-442a-ae27-5f1840ddb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57b70bf-9040-4169-8157-a3d9c3f56069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e6321a-c207-4145-bfc2-04b2905e58d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83b8ce-925b-4909-afa7-e171941c748c",
   "metadata": {},
   "source": [
    "Running this main function will demonstrate using Ray Tune to hyperparameter tune and save a Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b543f46f-a96b-48f5-94e8-b1203fdd41a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 20:12:38,192\tWARNING services.py:1832 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.52gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-06-16 20:12:38,401\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "2024-06-16 20:12:41,424\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-06-16 20:12:41,430\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-06-16 20:12:41,466\tINFO tensorboardx.py:178 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2024-06-16 20:12:41,466\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-16 20:19:59</td></tr>\n",
       "<tr><td>Running for: </td><td>00:07:18.16        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.0/186.6 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 2.000: -1.5778467945098877 | Iter 1.000: -1.688538500881195<br>Logical resource usage: 8.0/48 CPUs, 2.0/2 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_c8fdd_00000</td><td>TERMINATED</td><td>100.100.8.10:2044 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\">0.012325  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         72.2938</td><td style=\"text-align: right;\">1.60259</td><td style=\"text-align: right;\">    0.4051</td></tr>\n",
       "<tr><td>train_cifar_c8fdd_00001</td><td>TERMINATED</td><td>100.100.8.10:24115</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.00120253</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        188.377 </td><td style=\"text-align: right;\">1.60811</td><td style=\"text-align: right;\">    0.4041</td></tr>\n",
       "<tr><td>train_cifar_c8fdd_00002</td><td>TERMINATED</td><td>100.100.8.10:77348</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  16</td><td style=\"text-align: right;\">0.00594048</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        133.578 </td><td style=\"text-align: right;\">1.38654</td><td style=\"text-align: right;\">    0.5169</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=2044)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=2044)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=2044)\u001b[0m [1,  2000] loss: 2.034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">   loss</th><th>should_checkpoint  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_c8fdd_00000</td><td style=\"text-align: right;\">    0.4051</td><td style=\"text-align: right;\">1.60259</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_c8fdd_00001</td><td style=\"text-align: right;\">    0.4041</td><td style=\"text-align: right;\">1.60811</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_c8fdd_00002</td><td style=\"text-align: right;\">    0.5169</td><td style=\"text-align: right;\">1.38654</td><td>True               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=2044)\u001b[0m [2,  2000] loss: 1.677\n",
      "\u001b[2m\u001b[36m(func pid=2044)\u001b[0m [3,  2000] loss: 1.587\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [1,  2000] loss: 2.145\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [1,  4000] loss: 0.981\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [1,  6000] loss: 0.635\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [1,  8000] loss: 0.451\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [1, 10000] loss: 0.347\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [2,  2000] loss: 1.679\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [2,  4000] loss: 0.821\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [2,  6000] loss: 0.534\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [2,  8000] loss: 0.394\n",
      "\u001b[2m\u001b[36m(func pid=24115)\u001b[0m [2, 10000] loss: 0.313\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [1,  2000] loss: 1.977\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [1,  4000] loss: 0.826\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [2,  2000] loss: 1.518\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [2,  4000] loss: 0.744\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [3,  2000] loss: 1.414\n",
      "\u001b[2m\u001b[36m(func pid=77348)\u001b[0m [3,  4000] loss: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 20:19:59,645\tINFO tune.py:1148 -- Total run time: 438.22 seconds (438.11 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 64, 'l2': 16, 'lr': 0.005940475579325228, 'batch_size': 8}\n",
      "Best trial final validation loss: 1.3865361887693406\n",
      "Best trial final validation accuracy: 0.5169\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.5088\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
    "    best_checkpoint_data = best_checkpoint.to_dict()\n",
    "\n",
    "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "    \n",
    "    torch.save(best_trained_model.state_dict(), \"/home/cdsw/torch_model.pth\")\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of trials and GPUs here:\n",
    "    main(num_samples=3, max_num_epochs=3, gpus_per_trial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afac7fa-f7e5-4d69-9db9-0c6ffe11f3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
